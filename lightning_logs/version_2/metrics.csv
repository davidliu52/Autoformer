train_loss,epoch,step
-2.2827818393707275,0,99
-2.5790176391601562,1,199
-2.6712124347686768,2,299
-2.748136520385742,3,399
-2.8079276084899902,4,499
-2.849395751953125,5,599
-2.8782618045806885,6,699
-2.9064745903015137,7,799
-2.900913715362549,8,899
-2.912075996398926,9,999
-2.923966884613037,10,1099
-2.925959825515747,11,1199
-2.9368278980255127,12,1299
-2.934598684310913,13,1399
-2.9434049129486084,14,1499
-2.91728138923645,15,1599
-2.9333176612854004,16,1699
-2.9621176719665527,17,1799
-2.949003219604492,18,1899
-2.958355188369751,19,1999
-2.949026584625244,20,2099
-2.931246757507324,21,2199
-2.9376602172851562,22,2299
-2.9498648643493652,23,2399
-2.944254159927368,24,2499
-2.9422521591186523,25,2599
-2.9451591968536377,26,2699
-2.9455747604370117,27,2799
-2.943692207336426,28,2899
-2.9509756565093994,29,2999
-2.9615285396575928,30,3099
-2.9639663696289062,31,3199
-2.9614953994750977,32,3299
-2.964043378829956,33,3399
-2.96406626701355,34,3499
-2.963557481765747,35,3599
-2.9645204544067383,36,3699
-2.951432228088379,37,3799
-2.974855899810791,38,3899
-2.968207359313965,39,3999
-2.962343692779541,40,4099
-2.9573633670806885,41,4199
-2.9592785835266113,42,4299
-2.9579427242279053,43,4399
-2.9672751426696777,44,4499
-2.9645540714263916,45,4599
-2.9762210845947266,46,4699
-2.9718494415283203,47,4799
-2.9536080360412598,48,4899
-2.9759087562561035,49,4999
