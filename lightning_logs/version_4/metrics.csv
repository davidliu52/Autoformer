train_loss,step,epoch
-2.0301506519317627,99,0
-2.430682897567749,199,1
-2.6100428104400635,299,2
-2.6921210289001465,399,3
-2.7826032638549805,499,4
-2.851621150970459,599,5
-2.8763043880462646,699,6
-2.8977925777435303,799,7
-2.915241003036499,899,8
-2.915447950363159,999,9
-2.929486036300659,1099,10
-2.9373037815093994,1199,11
-2.9355859756469727,1299,12
-2.9262423515319824,1399,13
-2.9315292835235596,1499,14
-2.9338178634643555,1599,15
-2.94775390625,1699,16
-2.962517499923706,1799,17
-2.943127393722534,1899,18
-2.9625847339630127,1999,19
-2.9578135013580322,2099,20
-2.9616713523864746,2199,21
-2.943178653717041,2299,22
-2.9558911323547363,2399,23
-2.9547219276428223,2499,24
-2.953911542892456,2599,25
-2.958792209625244,2699,26
-2.954359531402588,2799,27
-2.9553709030151367,2899,28
-2.9594554901123047,2999,29
-2.959663152694702,3099,30
-2.9550952911376953,3199,31
-2.9593284130096436,3299,32
-2.9705426692962646,3399,33
-2.958400249481201,3499,34
-2.950146198272705,3599,35
-2.9655020236968994,3699,36
-2.963438034057617,3799,37
-2.954298734664917,3899,38
-2.960052251815796,3999,39
-2.9534103870391846,4099,40
-2.950976848602295,4199,41
-2.9468488693237305,4299,42
-2.958228826522827,4399,43
-2.958430528640747,4499,44
-2.9551475048065186,4599,45
-2.974539279937744,4699,46
-2.9707958698272705,4799,47
-2.968829870223999,4899,48
-2.9595370292663574,4999,49
